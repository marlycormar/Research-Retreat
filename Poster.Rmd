---
title: "2019 NSF-AGEP Research Exchange Retreat at Stanford University"
author: "Marly Cormar"
date: "`r Sys.Date()`"
output:
  pagedown::poster_relaxed:
    self_contained: false
bibliography: bibliography.bib
---


Applicability Domain [in]{.tiny} Data Science
================================================================================

which nobody knows how to pronounce (including Yihui himself, because it was adapted from the Japanese manga series Naruto by ) **xaringan**. After a few replies in the [Github issue](https://github.com/yihui/xaringan/issues/172), he realized there might be enough topics on **xaringan** for a short book. Accidentally, he invented a new development model for writing books: the Github-issue-driven book development.




[Authors]{.red}
================================================================================

We are a team of shinobi and kunoichi who wish to share the fun and secrets of the **xaringan** package with you.

::: member-cards
## Emi Tanaka

![Emi](https://avatars3.githubusercontent.com/u/7620319?s=400&v=4)

Lead author, and the ninja theme author

Emi laid out the first sketch of the book, which made Yihui believe that the book had been half-done.

## Joseph Casillas

![](https://avatars1.githubusercontent.com/u/1747068?s=400&v=4)

Contributor of **xaringan**

"Count me in," replied Joseph when Yihui asked who wanted to co-author the book.

## Eric Nantz

![](https://avatars0.githubusercontent.com/u/1043111?s=400&v=4)

Host of the R Podcast

Yihui is eager to know how much Eric's writing is better than his magnetic voice.

## Yihui Xie

![](https://avatars0.githubusercontent.com/u/163582?s=400&v=4)

Main author of **xaringan**

Yihui knows a bit about R/HTML/CSS/JS and wrote , which became a cornerstone of R Markdown.
:::

[All pictures above are from the authors' Github profiles. This poster was created via the R package [**pagedown**](https://github.com/rstudio/pagedown).]{.disclaimer}




[Applicability Domain]{.blue}
================================================================================

## Applicability Domain

The **Applicability Domain** (AD) expressess the scope and limitations of a model, i.e., the sample space for which the model is considered to be applicable.

The methods used to define the AD are based on the premise that a model's prediction is reliable if the sample for which a prediction is being made is _similar_ to the training set.

## Applicability Domain In Chemistry

QSARs (Quantitative Structure–Activity Relationships) are theoretical models that can be used to predict the physicochemical, biological and environmental properties of chemicals.

Since 2004, the Organisation for Economic Cooperation and Development (OECD) Member Countries adopted five principles for the validation of QSAR models for regulatory purposes, which include a "defined domain of applicability". This need is based on the fact that QSARs are reductionist models, which are inevitably associated with limitations in terms of the types of chemical structures, physicochemical properties and mechanisms of action for which they can generate reliable predictions [@Curr].

## Applicability Domain Outside Chemistry

Although there are methods currently used to establish the applicability domain of models, they have not been systematically studied as a whole and the ones that have, mostly use the outcome of the prediction to establish reliability.

Applicability domain methods measure the 'similarity' of new samples to the training set. As such, they do not depend on the outcome of the prediction, i.e., they are _unsupervised methods_.


[Methods]{.pink}
================================================================================

```{css, echo=FALSE}
#.section-4 {
#  background-image: url(https://upload.wikimedia.org/wikipedia/commons/7/7e/Mudra-Naruto-KageBunshin.svg) !important;
#  background-size: 40% !important;
#  background-position: right top !important;
#  background-repeat: no-repeat !important;
#}
```


## Principal Component Analysis

<!--
Because PCA is blind to the response, it is an unsupervised technique.
-->

It is a commonly used data reduction technique. This method seeks to find linear combinations of the predictors, known as principal components (PCs), which capture the most possible variance and are guaranteed to be uncorrelated.

The first PC is defined as the linear combination of the predictors that captures the most variability of all possible linear combinations. Then, subsequent PCs are derived such that these linear combinations capture the most remaining variability while also being uncorrelated with all previous PCs. The jth PC can be written as

$$PC_j = (a_{j1} \times \text{Predictor 1}) + \dots + (a_{jP} \times \text{Predictor P})$$

$P$ is the number of predictors. The coefficients $a_{ji}$ are called component weights and help us understand which predictors are most important to each PC [@APM].

<!--
While PCA delivers new predictors with desirable characteristics, it must be used with understanding and care. PCA seeks predictor-set variation without regard to any further understanding of the predictors (i.e., measurement scales or distributions) or to knowledge of the modeling objectives (i.e., response variable). Hence, without proper guidance, PCA can generate components that summarize characteristics of the data that are irrelevant to the underlying structure of the data and also to the ultimate modeling objective.

To help PCA avoid summarizing distributional differences and predictor scale information, it is best to first transform skewed predictors and then center and scale the predictors prior to performing PCA. Centering and scaling enables PCA to find the underlying relationships in the data without being influenced by the original measurement scales.

The second caveat of PCA is that it does not consider the modeling objective or response variable when summarizing variability. If the predictive relationship between the predictors and response is not connected to the predictors’ variability, then the derived PCs will not provide a suitable relationship with the response.
-->

## Hat Values

They measure the distance of a data point to the center of the training set distribution. For example, if the numeric training set matrix was $X_{n \times p}$, the hat matrix for the training set would be computed using

$$H = X^\intercal(X^\intercal X)^{-1}X$$
The corresponding hat values for the training set would be the diagonals of $H$. For a new sample, say a data vector $u_{p \times 1}$, the hat value would be

$$h = u^\intercal (X^\intercal X)^{-1}u$$
Hat values are less tolerant than PCA. For example, extremely correlated predictors will degrade the ability of the hat values to be effectively used. Also, since an inverse is used, there cannot be an linear dependencies within $X$.

## Similarity Statistics

Similarity statistics can be used to compare data sets where all of the predictors are binary. One of the most common measures is the Jaccard index. For sets $A$ and $B$, the Jaccard index is defined as

$$J(A, B) = \frac{|A \cap B|}{|A \cup B|}$$

For a training set of size $n$, there are $n$ similarity statistics for each new sample.  These can be summarized via the mean statistic or a quantile. In general, we want similarity to be low within the training set and high for new samples to be predicted.


[Examples by `applicable`]{.green}
================================================================================


## Principal Component Analysis

![The most well-known feature of **xaringan**: the random Moustache Karl (aka `yolo = TRUE`).](https://github.com/yihui/xaringan/releases/download/v0.0.2/karl-moustache.jpg)

## Hat Values


## Similarity Statistics

[Bibliography]{.yellow}
================================================================================

```{r, include=FALSE}
#knitr::write_bib(c('knitr', 'rmarkdown', 'xaringan'), 'packages.bib')
```
