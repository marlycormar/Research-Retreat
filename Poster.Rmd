---
title: "2019 NSF-AGEP Research Exchange Retreat at Stanford University"
author: "Marly Cormar"
date: "`r Sys.Date()`"
output:
  pagedown::poster_relaxed:
    self_contained: false
bibliography: bibliography.bib
---


Applicability Domain [in]{.tiny} Data Science
================================================================================

**About the Author**: Marly Cormar is a PhD Candidate in Mathematics at the University of Florida (UF). She also works as a software developer and data analyst for the College of Medicine at UF. Marly enjoys the pure flavor of researching mixed with the applied nature of developing applications. To see her full bio, you can go to her personal site *www.marlycormar.com*



[`applicable` Authors]{}
================================================================================

The R package `applicable` is currently in active development [@applicable]. It is a product of [Marly Cormar](https://www.marlycormar.com)'s Summer of 2019 internship with **RStudio** in collaboration with [Max Kuhn](https://github.com/topepo).

A shiny application comes with the package. It illustrates different methods used to define the applicability domain model. Check it out! https://marly-cormar.shinyapps.io/applicable/


::: member-cards
## Max Kuhn

![Max Kuhn](https://avatars1.githubusercontent.com/u/5731043?s=400&v=4)

Lead author

Statistician; software developer of many actively used R packages: `caret`, `recipes`, etc.

## Marly Cormar

![Marly Cormar](https://avatars1.githubusercontent.com/u/8559654?s=400&v=4)

Author and maintainer

PhD candidate in Mathematics; software developer; data analyst.

:::

[All pictures above are from the authors' GitHub profiles. This poster was created via the R package [**pagedown**](https://github.com/rstudio/pagedown).]{.disclaimer}




[Applicability Domain]{}
================================================================================

## Applicability Domain

The **Applicability Domain** (AD) expresses the scope and limitations of a model, i.e., the sample space for which the model is considered to be applicable.

The methods used to define the AD are based on the premise that a model's prediction is reliable if the sample for which a prediction is being made is _similar_ to the training set.

## Applicability Domain In Chemistry

QSARs (Quantitative Structure–Activity Relationships) are theoretical models that can be used to predict the physicochemical, biological and environmental properties of chemicals.

Since 2004, the Organization for Economic Cooperation and Development (OECD) Member Countries adopted five principles for the validation of QSAR models for regulatory purposes, which include a "defined domain of applicability". This need is based on the fact that QSARs are reductionist models, which are inevitably associated with limitations in terms of the types of chemical structures, physicochemical properties and mechanisms of action for which they can generate reliable predictions [@Curr].

## Applicability Domain Outside Chemistry

Although there are methods currently used to establish the applicability domain of models, they have not been systematically studied as a whole, and the ones that have mostly use the outcome of the prediction to establish reliability.

Applicability domain methods measure the 'similarity' of new samples to the training set. As such, they do not depend on the outcome of the prediction, i.e., they are _unsupervised methods_.


[Methods]{}
================================================================================

```{css, echo=FALSE}
#.section-4 {
#  background-image: url(https://upload.wikimedia.org/wikipedia/commons/7/7e/Mudra-Naruto-KageBunshin.svg) !important;
#  background-size: 40% !important;
#  background-position: right top !important;
#  background-repeat: no-repeat !important;
#}
```


## Principal Component Analysis

<!--
Because PCA is blind to the response, it is an unsupervised technique.
-->

It is a commonly used data reduction technique. This method seeks to find linear combinations of the predictors, known as principal components (PCs), which capture the most possible variance and are guaranteed to be uncorrelated.

The first PC is defined as the linear combination of the predictors that captures the most variability of all possible linear combinations. Then, subsequent PCs are derived such that these linear combinations capture the most remaining variability while also being uncorrelated with all previous PCs. The jth PC can be written as

$$PC_j = (a_{j1} \times \text{Predictor 1}) + \dots + (a_{jP} \times \text{Predictor P}),$$

where $P$ is the number of predictors. The coefficients $a_{ji}$ are called component weights and help us understand which predictors are most important to each PC [@APM].

<!--
While PCA delivers new predictors with desirable characteristics, it must be used with understanding and care. PCA seeks predictor-set variation without regard to any further understanding of the predictors (i.e., measurement scales or distributions) or to knowledge of the modeling objectives (i.e., response variable). Hence, without proper guidance, PCA can generate components that summarize characteristics of the data that are irrelevant to the underlying structure of the data and also to the ultimate modeling objective.

To help PCA avoid summarizing distributional differences and predictor scale information, it is best to first transform skewed predictors and then center and scale the predictors prior to performing PCA. Centering and scaling enables PCA to find the underlying relationships in the data without being influenced by the original measurement scales.

The second caveat of PCA is that it does not consider the modeling objective or response variable when summarizing variability. If the predictive relationship between the predictors and response is not connected to the predictors’ variability, then the derived PCs will not provide a suitable relationship with the response.
-->

## Hat Values

They measure the distance of a data point to the center of the training set distribution. For example, if the training set were the matrix $X_{n \times p}$, then the hat matrix for the training set would be computed using the formula

$$H = X^\intercal(X^\intercal X)^{-1}X.$$
The corresponding hat values for the training set would be the diagonal entries of $H$. For a new sample, say a data vector $u_{p \times 1}$, the hat value would be

$$h = u^\intercal (X^\intercal X)^{-1}u.$$
Hat values are less tolerant than PCA. For example, extremely correlated predictors will degrade the ability of the hat values to be effectively used. Also, since an inverse is used, there cannot be an linear dependencies within $X$.

## Similarity Statistics

Similarity statistics can be used to compare data sets where all of the predictors are binary. One of the most common measures is the Jaccard index. For sets $A$ and $B$, the Jaccard index is defined as

$$J(A, B) = \frac{|A \cap B|}{|A \cup B|}.$$

For a training set of size $n$, there are $n$ similarity statistics for each new sample.  These can be summarized via the mean statistic or a quantile. In general, we want similarity to be low within the training set and high for new samples to be predicted.


[Examples by `applicable`]{.green}
================================================================================


## Principal Component Analysis

![The most well-known feature of **xaringan**: the random Moustache Karl (aka `yolo = TRUE`).](https://github.com/yihui/xaringan/releases/download/v0.0.2/karl-moustache.jpg)

## Hat Values


## Similarity Statistics

[Bibliography]{.yellow}
================================================================================

```{r, include=FALSE}
#knitr::write_bib(c('knitr', 'rmarkdown', 'xaringan'), 'packages.bib')
```
